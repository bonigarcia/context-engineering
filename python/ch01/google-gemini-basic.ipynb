{"cells":[{"cell_type":"markdown","metadata":{"id":"vJct1b7Ip5lP"},"source":["# Google Gemini models with Python\n","\n","This notebook provides a simple example of how to use the Google Gemini models with Python."]},{"cell_type":"markdown","source":["## 1. Setup\n","\n","To use the Gogole Gemini API, you need to have an API key:\n","\n","1. Create or log into your Google account (a standard Google login)\n","2. Go to [Google AI Studio](https://aistudio.google.com/)\n","3. Open the [API Keys page](https://aistudio.google.com/app/apikey)\n","4. Click \"Create API key\", give it a name, and copy the key immediately.\n","5. Ensure your Google Cloud project has billing enabled, because Gemini API usage is pay-per-use. If prompted, you may need to link the API key to a Google Cloud project.\n","\n","Once you have the API key, you need to export its value as and environment variable called `GEMINI_API_KEY`. If this env is not defined, you will be asked for your API key (it will not be stored in the notebook)."],"metadata":{"id":"QaTx1r2xtcrm"}},{"cell_type":"code","source":["import os\n","from getpass import getpass\n","from google import genai\n","\n","# Ask for the API key interactively if not defined as env variable\n","if \"GEMINI_API_KEY\" not in os.environ:\n","    os.environ[\"GEMINI_API_KEY\"] = getpass(\"Enter your Gemini API key: \")\n","\n","client = genai.Client()"],"metadata":{"id":"Y6KqRHr0t7gx","executionInfo":{"status":"ok","timestamp":1764692737256,"user_tz":-60,"elapsed":176,"user":{"displayName":"Boni García","userId":"07441265877940459995"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## 2. Function to ask the model\n","\n","This section defines a helper function to send a user prompt to the model using the previously defined OpenAI client."],"metadata":{"id":"bTQ1T0dPt8NY"}},{"cell_type":"code","source":["def ask_model(prompt: str, model: str = \"gemini-2.5-flash\"):\n","    \"\"\"Send a text prompt to a Gemini model and return the text response\"\"\"\n","    response = client.models.generate_content(\n","        model=model,\n","        contents=prompt,\n","    )\n","    # response.text aggregates all text from the response\n","    return response.text"],"metadata":{"id":"f2JARYnDuAVw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Send user prompt\n","\n","This section sends and user prompt to the model using the function previously defined."],"metadata":{"id":"FKmAvGgluKJZ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFdaS5MBp5lU","executionInfo":{"status":"ok","timestamp":1764692292160,"user_tz":-60,"elapsed":5163,"user":{"displayName":"Boni García","userId":"07441265877940459995"}},"outputId":"fbceeeec-bbf6-464c-ac54-4d8d7f25121c"},"outputs":[{"output_type":"stream","name":"stdout","text":["User: How many tokens is your context window?\n","AI: I don't have a single, fixed \"context window\" in the same way some commercial APIs might declare a specific token limit. My underlying architecture, developed by Google, is designed to handle very long and complex conversations.\n","\n","This means I can generally retain a significant amount of information from our ongoing discussion and process lengthy inputs (like documents, articles, or large code snippets). While I don't have an exact, publicly stated token count for my *internal* context window, it's designed to be very large, often equivalent to **tens of thousands of tokens or even more in practical terms**.\n","\n","The practical limits you might encounter are more often due to the interface you're using or the overall system's processing capabilities, rather than a hard, internal token limit of the model itself.\n"]}],"source":["user_prompt = \"How many tokens is your context window?\"\n","response = ask_model(user_prompt)\n","print(\"User: \" + user_prompt)\n","print(\"AI: \" + response)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}