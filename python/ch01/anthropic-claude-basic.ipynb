{"cells":[{"cell_type":"markdown","metadata":{"id":"vJct1b7Ip5lP"},"source":["# Anthropic Claude models with Python\n","\n","This notebook provides a simple example of how to use the Anthropic Claude models with Python."]},{"cell_type":"markdown","source":["## 1. Setup\n","\n","To use the Gogole Gemini API, you need to have an API key:\n","\n","1. Create or log into your [Anthropic account](https://console.anthropic.com)\n","2. Go to [API Keys page](https://console.anthropic.com/settings/keys)\n","3. Click \"Create API key\", give it a name, and copy the key immediately.\n","4. Make sure your account has billing enabled, since Claude API usage is pay-per-use.\n","\n","Once you have the API key, you need to export its value as and environment variable called `ANTHROPIC_API_KEY`. If this env is not defined, you will be asked for your API key (it will not be stored in the notebook)."],"metadata":{"id":"QaTx1r2xtcrm"}},{"cell_type":"code","source":["!pip install anthropic\n","\n","import os\n","from getpass import getpass\n","from anthropic import Anthropic\n","\n","# Ask for the API key interactively if not defined as env variable\n","if \"ANTHROPIC_API_KEY\" not in os.environ:\n","    os.environ[\"ANTHROPIC_API_KEY\"] = getpass(\"Enter your Anthropic API key: \")\n","\n","client = Anthropic(api_key=os.environ.get(\"ANTHROPIC_API_KEY\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6KqRHr0t7gx","executionInfo":{"status":"ok","timestamp":1764693007759,"user_tz":-60,"elapsed":18615,"user":{"displayName":"Boni García","userId":"07441265877940459995"}},"outputId":"8f741b56-a386-4bcf-9c27-8dd52b6928f4"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting anthropic\n","  Downloading anthropic-0.75.0-py3-none-any.whl.metadata (28 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.11.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n","Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n","Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.12.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.12.3)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n","Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.41.4)\n","Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n","Downloading anthropic-0.75.0-py3-none-any.whl (388 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/388.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m378.9/388.2 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: anthropic\n","Successfully installed anthropic-0.75.0\n","Enter your Anthropic API key: ··········\n"]}]},{"cell_type":"markdown","source":["## 2. Function to ask the model\n","\n","This section defines a helper function to send a user prompt to the model using the previously defined OpenAI client."],"metadata":{"id":"bTQ1T0dPt8NY"}},{"cell_type":"code","source":["def ask_model(prompt: str, model: str = \"claude-3-haiku-20240307\"):\n","    \"\"\"Send a text prompt to a Gemini model and return the text response\"\"\"\n","    response = client.messages.create(\n","        model=model,\n","        max_tokens=1024,\n","        messages=[\n","            {\n","                \"role\": \"user\",\n","                \"content\": prompt,\n","            }\n","        ],\n","    )\n","\n","    # Claude responses are returned as a list of content blocks.\n","    # We concatenate any text blocks into a single string.\n","    parts = []\n","    for block in response.content:\n","        # Some SDK versions expose type as an attribute; others as dict key\n","        block_type = getattr(block, \"type\", None) or getattr(block, \"type_\", None) or getattr(block, \"dict\", lambda: {}).get(\"type\")\n","        if block_type == \"text\" or (hasattr(block, \"text\") and isinstance(block.text, str)):\n","            parts.append(block.text)\n","    # Fallback: if nothing was collected, just return str(response)\n","    return \"\".join(parts) if parts else str(response)"],"metadata":{"id":"f2JARYnDuAVw","executionInfo":{"status":"ok","timestamp":1764693372621,"user_tz":-60,"elapsed":8,"user":{"displayName":"Boni García","userId":"07441265877940459995"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## 3. Send user prompt\n","\n","This section sends and user prompt to the model using the function previously defined."],"metadata":{"id":"FKmAvGgluKJZ"}},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LFdaS5MBp5lU","executionInfo":{"status":"ok","timestamp":1764693375026,"user_tz":-60,"elapsed":784,"user":{"displayName":"Boni García","userId":"07441265877940459995"}},"outputId":"9fee601c-4095-44a3-e88a-1e7920867ab7"},"outputs":[{"output_type":"stream","name":"stdout","text":["User: How many tokens is your context window?\n","AI: I do not actually have a fixed context window size. I am an AI assistant created by Anthropic to be helpful, harmless, and honest. I don't have the same internal architecture as language models with sliding context windows. I respond to each message independently based on my training, without maintaining or accessing a persistent context.\n"]}],"source":["user_prompt = \"How many tokens is your context window?\"\n","response = ask_model(user_prompt)\n","print(\"User: \" + user_prompt)\n","print(\"AI: \" + response)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}