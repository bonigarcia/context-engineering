{"cells":[{"cell_type":"markdown","metadata":{"id":"I77poBt5hstw"},"source":["# Prompt stuffing example\n","\n","This notebook demonstrates **prompt stuffing**: injecting a large block of context directly into the prompt and observing how it changes the model's answer.\n","\n","We ask the **same question** in two scenarios:\n","\n","1. Without any background context.\n","2. With a long article stuffed into the prompt.\n","\n","You will see how additional context can ground the answer, but also how naive stuffing is not a scalable strategy compared to more structured context engineering techniques (RAG, memory, state, etc.)."],"id":"I77poBt5hstw"},{"cell_type":"markdown","metadata":{"id":"IfjM1MR-hsty"},"source":["## Setup: API key and client\n","\n","In this cell, you will be asked for your OpenAI API key (it will not be stored in the notebook). We then create a client using the `openai` Python library."],"id":"IfjM1MR-hsty"},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgi8lV43hstz","executionInfo":{"status":"ok","timestamp":1764183504484,"user_tz":-60,"elapsed":14929,"user":{"displayName":"Boni García","userId":"07441265877940459995"}},"outputId":"2cedf762-b221-416f-fa6d-f5cbec592d24"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API key: ··········\n"]}],"source":["import os\n","from getpass import getpass\n","from textwrap import dedent\n","from openai import OpenAI\n","\n","# Ask for the API key interactively (safer than hard-coding)\n","if \"OPENAI_API_KEY\" not in os.environ:\n","    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n","\n","client = OpenAI()\n","MODEL = \"gpt-4o-mini\"  # You can change this if needed"],"id":"rgi8lV43hstz"},{"cell_type":"markdown","metadata":{"id":"RzGRresghst0"},"source":["## Define the stuffed article and helper functions\n","\n","The article below simulates an internal note about context engineering. We then define two helper functions:\n","\n","* `ask_without_context` — asks the question with no background article.\n","* `ask_with_stuffed_context` — asks the same question but includes the article in the prompt (prompt stuffing)."],"id":"RzGRresghst0"},{"cell_type":"code","execution_count":7,"metadata":{"id":"PtkGaN3Vhst0","executionInfo":{"status":"ok","timestamp":1764191448623,"user_tz":-60,"elapsed":22,"user":{"displayName":"Boni García","userId":"07441265877940459995"}}},"outputs":[],"source":["ARTICLE = dedent(\n","    \"\"\"\n","    [Article: Internal note on context engineering]\n","\n","    Context engineering is the discipline of shaping and managing all the\n","    information that an AI model receives at inference time. Instead of\n","    treating a prompt as a single flat string, context engineering views\n","    the model's input as a deliberately assembled mix of complementary\n","    components: system instructions, user request, external knowledge,\n","    tools, memory, and state.\n","\n","    The first challenge is relevance. Because the context window is\n","    limited, engineers must decide which pieces of information are\n","    genuinely useful for solving the current task and which are noise.\n","    Overstuffing the prompt with loosely related details can dilute the\n","    model's attention and lead to generic or confused answers.\n","\n","    The second challenge is freshness. The model's parametric memory is\n","    frozen at training time, so external knowledge and memory pipelines\n","    must continuously feed updated information into the context window.\n","    This includes recent documents, user preferences, and live signals\n","    from tools or sensors.\n","\n","    The third challenge is structure. Raw text is often messy; effective\n","    context engineering requires predictable patterns such as templates,\n","    sections, and schemas. Well-structured context makes it easier for\n","    the model to understand roles, goals, constraints, and how retrieved\n","    evidence should be used.\n","\n","    In short, context engineering is about giving the model exactly the\n","    right information, in the right format, at the right moment—no more\n","    and no less.\n","    \"\"\"\n",").strip()\n","\n","\n","def ask_without_context(question: str) -> str:\n","    response = client.chat.completions.create(\n","        model=MODEL,\n","        messages=[\n","            {\n","                \"role\": \"system\",\n","                \"content\": (\n","                    \"You are a careful assistant. \"\n","                    \"If you do not have enough information, say \"\n","                    \"\\\"I don't know based on the context I have.\\\"\"\n","                ),\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": question,\n","            },\n","        ],\n","        temperature=0.3,\n","    )\n","    return response.choices[0].message.content\n","\n","\n","def ask_with_stuffed_context(article: str, question: str) -> str:\n","    user_content = dedent(\n","        f\"\"\"\n","        I will give you an internal article as context. Use ONLY that\n","        article to answer the question. If something is not supported\n","        by the article, say you don't know.\n","\n","        === Article begins ===\n","        {article}\n","        === Article ends ===\n","\n","        Question:\n","        {question}\n","        \"\"\"\n","    ).strip()\n","\n","    response = client.chat.completions.create(\n","        model=MODEL,\n","        messages=[\n","            {\n","                \"role\": \"system\",\n","                \"content\": (\n","                    \"You are a careful assistant. \"\n","                    \"Base your answer strictly on the provided article.\"\n","                ),\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": user_content,\n","            },\n","        ],\n","        temperature=0.3,\n","    )\n","    return response.choices[0].message.content"],"id":"PtkGaN3Vhst0"},{"cell_type":"markdown","metadata":{"id":"IGDQomhRhst1"},"source":["## Run both scenarios\n","\n","We now ask the same question twice:\n","\n","1. Without any article.\n","2. With the article stuffed into the prompt.\n","\n","Compare the outputs to see how the background context changes the model's answer."],"id":"IGDQomhRhst1"},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K2xu3-5Hhst1","executionInfo":{"status":"ok","timestamp":1764191456656,"user_tz":-60,"elapsed":5599,"user":{"displayName":"Boni García","userId":"07441265877940459995"}},"outputId":"0900d3ea-0d84-4d13-a3d8-6b99cf26e1f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["=== Scenario 1: No stuffed context ===\n","\n","I don't know based on the context I have.\n","\n","========================================================================\n","\n","=== Scenario 2: Question with stuffed article ===\n","\n","According to the article, the three main challenges of context engineering are relevance, freshness, and structure.\n","\n","1. **Relevance**: Engineers must determine which pieces of information are genuinely useful for solving the current task, as overstuffing the prompt with loosely related details can dilute the model's attention and lead to generic or confused answers.\n","\n","2. **Freshness**: The model's parametric memory is fixed at training time, so it is essential to continuously feed updated information into the context window. This includes recent documents, user preferences, and live signals from tools or sensors.\n","\n","3. **Structure**: Effective context engineering requires predictable patterns such as templates, sections, and schemas because raw text is often messy. Well-structured context helps the model understand roles, goals, constraints, and how to use retrieved evidence properly.\n"]}],"source":["question = (\n","    \"According to the article, what are the three main challenges of \"\n","    \"context engineering, and how are they described?\"\n",")\n","\n","print(\"=== Scenario 1: No stuffed context ===\\n\")\n","answer1 = ask_without_context(question)\n","print(answer1)\n","\n","print(\"\\n\" + \"=\" * 72 + \"\\n\")\n","\n","print(\"=== Scenario 2: Question with stuffed article ===\\n\")\n","answer2 = ask_with_stuffed_context(ARTICLE, question)\n","print(answer2)"],"id":"K2xu3-5Hhst1"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}